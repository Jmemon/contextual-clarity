# Round 1 Refinements — Clarification Result

Reconstructed from the clarification loop session (`2f5ec12c`, 2026-02-18). The original 20 tasks from `notes-extracted.md` were discussed, refined, merged, and consolidated into 12 implementation tasks.

---

## 1. Per-Session Recall Point State (`checklist`)

Replace the linear `currentPointIndex` in `SessionEngine` with a `Map<pointId, 'pending' | 'recalled'>`. On every user message, the evaluator receives ALL unchecked points and reports which ones (zero or more) were demonstrated — regardless of order.

**Tutor behavior:** Probes points in the original order by default, but is not rigid. If the user mentions point 3 unprompted, the system checks it off and the tutor moves on to the next unchecked point. The tutor never fights the user's natural flow.

**Prompt change:** Instead of "Current Recall Point" + "Upcoming Points," the tutor sees "Unchecked Points (probe these)" with a suggested order. Instruction: "Probe in the order listed, but if the user demonstrates any out of order, accept it and move on."

**Current code:**
- `SessionEngine.currentPointIndex` tracks position linearly (`session-engine.ts`)
- `processUserMessage()` only evaluates the current point
- `buildSocraticTutorPrompt()` in `socratic-tutor.ts` builds separate "Current Point" and "Upcoming Points" sections

**Powers:** visual progress (#4), auto-end (#3), prevents re-asking recalled points (#2)

---

## 2. Continuous Evaluator with Feedback Field (`evaluator`)

*Merges original #2 (batch/out-of-order recall), #3 ("be more precise" nudges)*

The evaluator becomes middleware in the conversation loop, not a scorer at the end. It runs on **every user message** — continuous, not triggered by phrases or message counts.

**New flow:**
1. User speaks
2. Evaluator runs against all unchecked points → returns which were hit, confidence per point, AND a feedback/guidance string
3. That feedback string is injected into the tutor's context before generating the next response
4. Tutor's next response is shaped by what the evaluator observed

**Example feedback:** *"User recalled point 3 (adenosine receptor blocking) accurately. Their description of tolerance was close but imprecise — they said 'your body adjusts' without mentioning upregulation of receptors. Points 1 and 2 remain unchecked."*

The tutor sees this guidance, so it knows to nudge toward precision on tolerance and move to probing the next unchecked point — without the user seeing any of this machinery. "Be more precise" is just one flavor of feedback. Others: "they got the concept but used the wrong term," "they described the mechanism but missed the key detail."

**Two evaluator modes:**
- **Recall mode** — full feedback, steers the tutor on what to probe/nudge
- **Rabbit hole mode** — just silently tags recall points as hit, no steering feedback (the rabbit hole agent isn't probing, it's following user curiosity)

**Kills:** `EVALUATION_TRIGGER_PHRASES`, `trigger_eval` WebSocket message type, the "I've got it!" button's evaluation purpose

**Current code:**
- `evaluateCurrentPoint()` in `session-engine.ts` calls `evaluator.evaluate(currentPoint, messages)` — singular point
- `buildRecallEvaluatorPrompt()` in `recall-evaluator.ts` receives one `recallPointContent`
- `EVALUATION_TRIGGER_PHRASES` in `types.ts`

---

## 3. Sessions Auto-End + Leave Session (`auto-end`)

When all points in the checklist are `recalled`, the session signals completion with an **overlay** — not a hard stop.

**Overlay behavior:**
- Appears when all points are hit
- Shows something like "You've covered everything!"
- Has a "Continue Discussion" option — if the user is enjoying it, they can keep going
- Or they can press exit

**"Leave Session" button:**
- Always visible (replaces both "I've got it!" and "End Session")
- **Pauses** the session — user can jump back in later
- Not a termination, a pause

**During rabbit holes:** If the user is mid-rabbit-hole when the last point gets checked off, the system doesn't interrupt. It notes completion and lets them exit the rabbit hole naturally, then shows the completion overlay.

**Current code:**
- `SessionControls.tsx` renders "I've got it!" and "End Session" buttons
- `trigger_eval` WebSocket message type

---

## 4. Visual Progress (`progress`)

**Display elements:**
- Small anonymous icons (one per point) that fill in as points are recalled — the user knows *how many* they've hit but NOT *which topics* remain. No information about what the points actually are.
- Satisfying sound on each recall
- Time elapsed
- Running list of rabbit hole names — short 2-4 word labels generated by Haiku as rabbit holes are explored during the session

**During rabbit holes:** Progress UI is **hidden**. Evaluation still runs silently (points can be recalled during exploration per #2 rabbit hole mode), but recalled-during-rabbit-hole points only appear in the progress after exiting the rabbit hole. Rabbit holes should feel exploratory — no pressure.

**Current code:**
- `SessionProgress.tsx` shows "X / Y points" with a progress bar tracking the linear index

---

## 5. Structured Actions / Event Protocol (`events`)

**Deferred definition.** The full event list will emerge as other tasks solidify the shape of the recall session.

The session engine becomes the driver of recall session agents — it calls them, they modify context and generate output. Events will include things like:
- `recall_point_recalled` — UI renders checkmark animation + sound, not LLM text
- `rabbithole_detected` — UI renders a prompt
- `rabbithole_entered` / `rabbithole_exited` — UI transitions
- `session_complete_overlay` — triggers completion overlay
- Likely more as the system takes shape

The tutor LLM stops generating praise text or meta-commentary about progress — all handled by UI reacting to structured actions.

**Build last** so all events can be defined from the full picture.

---

## 6. Tone Overhaul (`tone`)

*Merges original #7 (less verbose), #8 (remove encouraging feedback), #9 (remove encourage elaboration), #18 (fix "I recall nothing")*

Complete prompt rewrite of `buildSocraticMethodSection()` and `buildGuidelinesSection()` in `socratic-tutor.ts`. Targets the universal agent's prompt (#7), so write the agent first, then tune its voice.

**Tone:** Direct, conversational, not verbose. "Like talking to a knowledgeable friend who asks good questions." No lists, no bullet points, no "Great question!" preamble. Short sentences that get the user thinking in the right direction.

**Specific removals:**
- "Celebrate recall warmly" / encouraging feedback section → removed. System handles positive reinforcement through UI (#5 structured actions)
- "Encourage elaboration" → removed. The user can go down rabbit holes if they choose; the system shouldn't push them. When a point is recalled, acknowledge it and move to the next unchecked point.

**Addition:** Handle "I recall nothing" / "I don't remember" as a valid user response. Begin with broad hints to help them start recalling. Not meta-commentary, not confusion about whether the session has started.

**Current code:**
- `buildSocraticMethodSection()` principle #4 encourages elaboration
- `buildGuidelinesSection()` includes encouraging feedback
- No handling for "I recall nothing" phrasing

---

## 7. Universal Discussion Agent (`universal-agent`)

Replace per-set `discussionSystemPrompt` with one universal discussion agent prompt that works across all domains. The recall points themselves provide enough domain context.

**`discussionSystemPrompt` field becomes:** Optional supplementary guidelines (e.g., "this topic uses a lot of chemistry terminology" or "the user tends to confuse X with Y"). Not the backbone of the prompt.

**Auto-generation of per-set guidelines from user interactions:** Later feature. For now, just the universal agent.

**Requirement:** All conversations MUST be stored (they already are in `session_messages`). This is the data we'll mine later to auto-generate per-set guidelines.

**Current code:**
- `buildCustomPromptSection()` in `socratic-tutor.ts` injects `recallSet.discussionSystemPrompt` as the first prompt section
- Each seed file has a custom prompt per set

---

## 8. Rabbit Holes as Separate UX Mode (`rabbitholes`)

A first-class UX mode, not just detection-and-redirect.

**Architecture:**
- When a rabbit hole is detected and user opts in, the system spins up a **separate LLM context** — its own conversation history, its own system prompt (exploratory, not Socratic)
- Main recall session context is **frozen**
- User's messages go to the rabbit hole agent, not the recall tutor
- On exit: rabbit hole context is discarded (but stored in DB for later analysis), user returns to main session exactly where they left off

**UX:**
- Visual transition — color scheme shifts, progress indicators hide
- Entry is a prompt ("Want to explore [topic]?")
- Exit button always visible ("Return to session" or similar)

**Agent personality:** Driven by what the user appears interested in. Conversational and curious, not Socratic. Can go deep, reference source material (#11). Completely different from the recall tutor.

**Rabbit hole detection stays separate** from the evaluator — dedicated `RabbitholeDetector` class remains.

**Evaluator in rabbit hole mode:** Runs with a different prompt. Just silently tags recall points as hit — no steering feedback to the agent. The rabbit hole agent doesn't need guidance from the evaluator. Feedback is stripped down: "point X was recalled" → update checklist quietly.

**Current code:**
- `rabbithole-detector.ts` detects tangents and instructs tutor to redirect
- `RabbitholeDetector` class in `core/analysis/` tracks active rabbit holes for metrics only

---

## 9. Transcription Processing Pipeline (`transcription`)

*Merges original #12 (terminology correction) + #13 (math/code/formula rendering)*

One pipeline: **raw transcription → post-processing → display-ready text.** Same Haiku call potentially — different instructions depending on what's detected.

**Pipeline steps:**
1. **Terminology correction** (simpler) — Haiku fixes domain words against the recall set's vocabulary. "My selenium networks" → "mycelium networks", "phosphor and hydride" → "phosphoanhydride"
2. **Notation detection and conversion** (more complex) — Haiku identifies when the user is articulating math, code, or formulas in natural language and converts to appropriate notation (LaTeX for math, code blocks for code). Natural language versions of code/math get transmuted into computer-understood representations.
3. **Rendering** — Frontend renders LaTeX via KaTeX, code via markdown code blocks

**Input:** Recall set terminology extracted from all recall points' content and context fields, passed alongside the transcription. Fast, cheap, latency-sensitive.

**Output:** Corrected text is what the user sees on screen and what enters the LLM context.

---

## 10. Voice-First Input (`voice`)

Speaking forces genuine recall from memory — typing allows a certain mindlessness.

**Transcription service:** Proper transcription service (not browser Web Speech API). Reliable, consistent.

**Input UI:** Thin, horizontally oriented input bar. Mic icon on the right side. NOT a giant mic button.
1. Tap mic → waveform indicator slides by as they speak
2. Tap mic again → done recording
3. Transcribed + processed message appears for review
4. **Swipe up** to approve/send

**Correction mode:** If they don't approve the message, tap mic again — but now their speech is interpreted as *instructions on how to edit the displayed message* ("change the second part to say X" or "I said mycorrhizal not micro rival"). Haiku applies the edit → updated message appears for approval again. Repeat until swipe up.

**Single-exchange view replacing chat scroll:**
- No scrolling conversation history
- At any moment, user sees: AI's current question/text + input bar
- After approval: user's message animates upward into the conversation area → loading indicator → AI's new response generates and replaces everything
- Each exchange replaces the last

**Current code:** Entirely text-based. No voice/audio infrastructure exists.

---

## 11. Source Resources with Images (`resources`)

*Merges original #15 (raw source resources) + #16 (image support)*

**In scope for this round.**

**Resources:** New `resources` table or field linked to recall sets — articles, excerpts, book passages that contain the source material recall points were derived from. For testing, find short source materials for existing recall sets.

**Agents can:** Reference, quote, and display resources during sessions. The rabbit hole agent (#8) uses them for deeper exploration. The transcription pipeline (#9) uses terminology from them.

**Images:** Part of resources. Can be linked to specific recall points. Displaying an image is a structured action (#5) — the engine tells the UI to display it, not the LLM generating markdown. Example: showing a movie frame for Rule of Thirds when the user recalls the relevant aspect.

---

## 12. Rename "AI Tutor" to "Agent" (`rename`)

Replace "AI Tutor" label with "Agent" in the UI.

**Current code:**
- `MessageList.tsx` line 73: `{isUser ? 'You' : 'AI Tutor'}`
- `StreamingMessage.tsx` line 47: `AI Tutor`

---

## Deferred

- **Mobile version** (#19) — not in this round, but voice-first design should consider mobile from the start
- **User-suggested software changes** (#20) — product feature for later
- **Auto-generation of per-set guidelines** — later, once enough conversations are stored
